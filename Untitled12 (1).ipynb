{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p4Dm91fGaKz",
        "outputId": "21d433b7-b461-4571-8e18-95a512809103"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean squared error: 250000.0\n",
            "R-squared: 0.9876543209876543\n"
          ]
        }
      ],
      "source": [
        "#liner regression salary\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Ye dummy data hai\n",
        "X = np.array([[10], [11], [12], [13], [14], [15]])\n",
        "y = np.array([30000, 32000, 35000, 38000, 41000, 45000])\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "#split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Model train ho raha hai\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the salary for testing data\n",
        "predicted_salary = model.predict(X_test)\n",
        "\n",
        "#Accuracy measure\n",
        "mse = mean_squared_error(y_test, predicted_salary)\n",
        "r2 = r2_score(y_test, predicted_salary)\n",
        "print(\"Mean squared error:\", mse)\n",
        "print(\"R-squared:\", r2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mq0gX-ZJGik9",
        "outputId": "96bbd182-6358-4dfa-f5f5-89f5542231a3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "\n",
        "model = SVC()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyxLl_dIHumC",
        "outputId": "48d3ad29-ed57-4a4d-b845-009b9b3d95ef"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = [1, 1]\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Training data for an AND gate\n",
        "inputs = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "outputs = [0, 0, 0, 1]\n",
        "\n",
        "for i in range(len(inputs)):\n",
        "\n",
        "    weighted_sum = sum(weights[j] * inputs[i][j] for j in range(len(weights)))\n",
        "\n",
        "    # Update the weights using the Hebbian learning rule\n",
        "    for j in range(len(weights)):\n",
        "        weights[j] += learning_rate * inputs[i][j] * (outputs[i] - weighted_sum)\n",
        "\n",
        "def predict(inputs):\n",
        "    weighted_sum = weights[0] * inputs[0] + weights[1] * inputs[1]\n",
        "    return 1 if weighted_sum > 1 else 0\n",
        "\n",
        "print(predict([0, 0]))  # Output: 0\n",
        "print(predict([0, 1]))  # Output: 0\n",
        "print(predict([1, 0]))  # Output: 0\n",
        "print(predict([1, 1]))  # Output: 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76jJsmz8H7kI",
        "outputId": "7e65e2b7-69c0-4fba-a17b-701585ae511c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = [1, 1]\n",
        "learning_rate = 0.1\n",
        "\n",
        "inputs = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "outputs = [0, 1, 1, 1]\n",
        "\n",
        "for i in range(len(inputs)):\n",
        "\n",
        "    weighted_sum = sum(weights[j] * inputs[i][j] for j in range(len(weights)))\n",
        "\n",
        "    # Update the weights using the Hebbian learning rule\n",
        "    for j in range(len(weights)):\n",
        "        weights[j] += learning_rate * inputs[i][j] * (outputs[i] - weighted_sum)\n",
        "\n",
        "def predict(inputs):\n",
        "    weighted_sum = weights[0] * inputs[0] + weights[1] * inputs[1]\n",
        "    return 1 if weighted_sum > 0 else 0\n",
        "\n",
        "print(predict([0, 0]))  # Output: 0\n",
        "print(predict([0, 1]))  # Output: 0\n",
        "print(predict([1, 0]))  # Output: 0\n",
        "print(predict([1, 1]))  # Output: 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8DK5va2ISIm",
        "outputId": "2e9c2c46-3c4a-4519-cbef-00e7384514a1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "1\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the inputs and outputs for the OR gate\n",
        "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "outputs = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Initialize the weights randomly\n",
        "weights = np.random.randn(2)\n",
        "\n",
        "# Set the learning rate\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Train the perceptron\n",
        "for epoch in range(100):\n",
        "    for i in range(len(inputs)):\n",
        "        # Calculate the weighted sum of the inputs\n",
        "        weighted_sum = np.dot(inputs[i], weights)\n",
        "\n",
        "        # Apply the activation function (e.g., Heaviside step function)\n",
        "        activation = 1 if weighted_sum > 1 else 0\n",
        "\n",
        "        # Update the weights using the Perceptron learning rule\n",
        "        error = outputs[i] - activation\n",
        "        weights += learning_rate * error * inputs[i]\n",
        "\n",
        "# Make predictions on new inputs\n",
        "def predict(inputs):\n",
        "    weighted_sum = np.dot(inputs, weights)\n",
        "    activation = 1 if weighted_sum > 1 else 0\n",
        "    return activation\n",
        "\n",
        "print(predict([0, 0]))  # Output: 0\n",
        "print(predict([0, 1]))  # Output: 0\n",
        "print(predict([1, 0]))  # Output: 0\n",
        "print(predict([1, 1]))  # Output: 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UV0D5z-BIkZo",
        "outputId": "479b06f2-17a8-49dd-d8a4-ba9e9ed1a084"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the inputs and outputs for the OR gate\n",
        "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "outputs = np.array([0, 1, 1, 1])\n",
        "\n",
        "# Initialize the weights randomly\n",
        "weights = np.random.randn(2)\n",
        "\n",
        "# Set the learning rate\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Train the perceptron\n",
        "for epoch in range(100):\n",
        "    for i in range(len(inputs)):\n",
        "        # Calculate the weighted sum of the inputs\n",
        "        weighted_sum = np.dot(inputs[i], weights)\n",
        "\n",
        "        # Apply the activation function (e.g., Heaviside step function)\n",
        "        activation = 1 if weighted_sum > 0 else 0\n",
        "\n",
        "        # Update the weights using the Perceptron learning rule\n",
        "        error = outputs[i] - activation\n",
        "        weights += learning_rate * error * inputs[i]\n",
        "\n",
        "# Make predictions on new inputs\n",
        "def predict(inputs):\n",
        "    weighted_sum = np.dot(inputs, weights)\n",
        "    activation = 1 if weighted_sum > 0 else 0\n",
        "    return activation\n",
        "\n",
        "print(predict([0, 0]))  # Output: 0\n",
        "print(predict([0, 1]))  # Output: 1\n",
        "print(predict([1, 0]))  # Output: 1\n",
        "print(predict([1, 1]))  # Output: 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOgrewz_IyKs",
        "outputId": "fd6f8f4a-b326-4a0d-c487-9de9817fad3d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "1\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Standardize the data\n",
        "X_std = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
        "\n",
        "# Create a PCA instance with 2 components\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Fit the PCA instance to the data\n",
        "X_reduced = pca.fit_transform(X_std)\n",
        "\n",
        "# Print the explained variance ratio for each principal component\n",
        "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
        "\n",
        "# Print the first two principal components for each data point\n",
        "print(f\"First two principal components:- {X_reduced}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXHH0gGBI3-P",
        "outputId": "fb7aade1-89cc-43d8-fe39-962b4210e959"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained variance ratio: [0.72962445 0.22850762]\n",
            "First two principal components:- [[-2.26470281  0.4800266 ]\n",
            " [-2.08096115 -0.67413356]\n",
            " [-2.36422905 -0.34190802]\n",
            " [-2.29938422 -0.59739451]\n",
            " [-2.38984217  0.64683538]\n",
            " [-2.07563095  1.48917752]\n",
            " [-2.44402884  0.0476442 ]\n",
            " [-2.23284716  0.22314807]\n",
            " [-2.33464048 -1.11532768]\n",
            " [-2.18432817 -0.46901356]\n",
            " [-2.1663101   1.04369065]\n",
            " [-2.32613087  0.13307834]\n",
            " [-2.2184509  -0.72867617]\n",
            " [-2.6331007  -0.96150673]\n",
            " [-2.1987406   1.86005711]\n",
            " [-2.26221453  2.68628449]\n",
            " [-2.2075877   1.48360936]\n",
            " [-2.19034951  0.48883832]\n",
            " [-1.898572    1.40501879]\n",
            " [-2.34336905  1.12784938]\n",
            " [-1.914323    0.40885571]\n",
            " [-2.20701284  0.92412143]\n",
            " [-2.7743447   0.45834367]\n",
            " [-1.81866953  0.08555853]\n",
            " [-2.22716331  0.13725446]\n",
            " [-1.95184633 -0.62561859]\n",
            " [-2.05115137  0.24216355]\n",
            " [-2.16857717  0.52714953]\n",
            " [-2.13956345  0.31321781]\n",
            " [-2.26526149 -0.3377319 ]\n",
            " [-2.14012214 -0.50454069]\n",
            " [-1.83159477  0.42369507]\n",
            " [-2.61494794  1.79357586]\n",
            " [-2.44617739  2.15072788]\n",
            " [-2.10997488 -0.46020184]\n",
            " [-2.2078089  -0.2061074 ]\n",
            " [-2.04514621  0.66155811]\n",
            " [-2.52733191  0.59229277]\n",
            " [-2.42963258 -0.90418004]\n",
            " [-2.16971071  0.26887896]\n",
            " [-2.28647514  0.44171539]\n",
            " [-1.85812246 -2.33741516]\n",
            " [-2.5536384  -0.47910069]\n",
            " [-1.96444768  0.47232667]\n",
            " [-2.13705901  1.14222926]\n",
            " [-2.0697443  -0.71105273]\n",
            " [-2.38473317  1.1204297 ]\n",
            " [-2.39437631 -0.38624687]\n",
            " [-2.22944655  0.99795976]\n",
            " [-2.20383344  0.00921636]\n",
            " [ 1.10178118  0.86297242]\n",
            " [ 0.73133743  0.59461473]\n",
            " [ 1.24097932  0.61629765]\n",
            " [ 0.40748306 -1.75440399]\n",
            " [ 1.0754747  -0.20842105]\n",
            " [ 0.38868734 -0.59328364]\n",
            " [ 0.74652974  0.77301931]\n",
            " [-0.48732274 -1.85242909]\n",
            " [ 0.92790164  0.03222608]\n",
            " [ 0.01142619 -1.03401828]\n",
            " [-0.11019628 -2.65407282]\n",
            " [ 0.44069345 -0.06329519]\n",
            " [ 0.56210831 -1.76472438]\n",
            " [ 0.71956189 -0.18622461]\n",
            " [-0.0333547  -0.43900321]\n",
            " [ 0.87540719  0.50906396]\n",
            " [ 0.35025167 -0.19631173]\n",
            " [ 0.15881005 -0.79209574]\n",
            " [ 1.22509363 -1.6222438 ]\n",
            " [ 0.1649179  -1.30260923]\n",
            " [ 0.73768265  0.39657156]\n",
            " [ 0.47628719 -0.41732028]\n",
            " [ 1.2341781  -0.93332573]\n",
            " [ 0.6328582  -0.41638772]\n",
            " [ 0.70266118 -0.06341182]\n",
            " [ 0.87427365  0.25079339]\n",
            " [ 1.25650912 -0.07725602]\n",
            " [ 1.35840512  0.33131168]\n",
            " [ 0.66480037 -0.22592785]\n",
            " [-0.04025861 -1.05871855]\n",
            " [ 0.13079518 -1.56227183]\n",
            " [ 0.02345269 -1.57247559]\n",
            " [ 0.24153827 -0.77725638]\n",
            " [ 1.06109461 -0.63384324]\n",
            " [ 0.22397877 -0.28777351]\n",
            " [ 0.42913912  0.84558224]\n",
            " [ 1.04872805  0.5220518 ]\n",
            " [ 1.04453138 -1.38298872]\n",
            " [ 0.06958832 -0.21950333]\n",
            " [ 0.28347724 -1.32932464]\n",
            " [ 0.27907778 -1.12002852]\n",
            " [ 0.62456979  0.02492303]\n",
            " [ 0.33653037 -0.98840402]\n",
            " [-0.36218338 -2.01923787]\n",
            " [ 0.28858624 -0.85573032]\n",
            " [ 0.09136066 -0.18119213]\n",
            " [ 0.22771687 -0.38492008]\n",
            " [ 0.57638829 -0.1548736 ]\n",
            " [-0.44766702 -1.54379203]\n",
            " [ 0.25673059 -0.5988518 ]\n",
            " [ 1.84456887  0.87042131]\n",
            " [ 1.15788161 -0.69886986]\n",
            " [ 2.20526679  0.56201048]\n",
            " [ 1.44015066 -0.04698759]\n",
            " [ 1.86781222  0.29504482]\n",
            " [ 2.75187334  0.8004092 ]\n",
            " [ 0.36701769 -1.56150289]\n",
            " [ 2.30243944  0.42006558]\n",
            " [ 2.00668647 -0.71143865]\n",
            " [ 2.25977735  1.92101038]\n",
            " [ 1.36417549  0.69275645]\n",
            " [ 1.60267867 -0.42170045]\n",
            " [ 1.8839007   0.41924965]\n",
            " [ 1.2601151  -1.16226042]\n",
            " [ 1.4676452  -0.44227159]\n",
            " [ 1.59007732  0.67624481]\n",
            " [ 1.47143146  0.25562182]\n",
            " [ 2.42632899  2.55666125]\n",
            " [ 3.31069558  0.01778095]\n",
            " [ 1.26376667 -1.70674538]\n",
            " [ 2.0377163   0.91046741]\n",
            " [ 0.97798073 -0.57176432]\n",
            " [ 2.89765149  0.41364106]\n",
            " [ 1.33323218 -0.48181122]\n",
            " [ 1.7007339   1.01392187]\n",
            " [ 1.95432671  1.0077776 ]\n",
            " [ 1.17510363 -0.31639447]\n",
            " [ 1.02095055  0.06434603]\n",
            " [ 1.78834992 -0.18736121]\n",
            " [ 1.86364755  0.56229073]\n",
            " [ 2.43595373  0.25928443]\n",
            " [ 2.30492772  2.62632347]\n",
            " [ 1.86270322 -0.17854949]\n",
            " [ 1.11414774 -0.29292262]\n",
            " [ 1.2024733  -0.81131527]\n",
            " [ 2.79877045  0.85680333]\n",
            " [ 1.57625591  1.06858111]\n",
            " [ 1.3462921   0.42243061]\n",
            " [ 0.92482492  0.0172231 ]\n",
            " [ 1.85204505  0.67612817]\n",
            " [ 2.01481043  0.61388564]\n",
            " [ 1.90178409  0.68957549]\n",
            " [ 1.15788161 -0.69886986]\n",
            " [ 2.04055823  0.8675206 ]\n",
            " [ 1.9981471   1.04916875]\n",
            " [ 1.87050329  0.38696608]\n",
            " [ 1.56458048 -0.89668681]\n",
            " [ 1.5211705   0.26906914]\n",
            " [ 1.37278779  1.01125442]\n",
            " [ 0.96065603 -0.02433167]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Load the Iris dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Reshape the data to fit the CNN input format\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\n",
        "# Define the CNN architecture\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print('Loss:', loss)\n",
        "print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EmKK-PMJNXK",
        "outputId": "897e95e1-b677-499e-f1c5-e903e2443cb4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 56s 29ms/step - loss: 0.1294 - accuracy: 0.9607 - val_loss: 0.0614 - val_accuracy: 0.9807\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 53s 28ms/step - loss: 0.0432 - accuracy: 0.9869 - val_loss: 0.0325 - val_accuracy: 0.9897\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.0293 - accuracy: 0.9911 - val_loss: 0.0340 - val_accuracy: 0.9886\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 60s 32ms/step - loss: 0.0209 - accuracy: 0.9931 - val_loss: 0.0359 - val_accuracy: 0.9883\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 62s 33ms/step - loss: 0.0159 - accuracy: 0.9950 - val_loss: 0.0276 - val_accuracy: 0.9911\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.0125 - accuracy: 0.9959 - val_loss: 0.0336 - val_accuracy: 0.9902\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.0099 - accuracy: 0.9966 - val_loss: 0.0370 - val_accuracy: 0.9901\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 50s 27ms/step - loss: 0.0087 - accuracy: 0.9973 - val_loss: 0.0348 - val_accuracy: 0.9905\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 0.0067 - accuracy: 0.9979 - val_loss: 0.0359 - val_accuracy: 0.9905\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 53s 29ms/step - loss: 0.0069 - accuracy: 0.9977 - val_loss: 0.0366 - val_accuracy: 0.9913\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0366 - accuracy: 0.9913\n",
            "Loss: 0.036634791642427444\n",
            "Accuracy: 0.9912999868392944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iDFNDaB8M42W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}